%!TEX root = ../thesis.tex

\chapter{Experiments and Results}
\label{cha:experimentsandresults}

In this chapter, we are going to explain the setup required to run the experiments,
the datasets used, and the quantitative and qualitative results resulting from these experiments.

\section{Setup}
\subsection{Workstation}

The main developing tool during this thesis has been a laptop which has been used for various tasks, such as writing code, debugging, reading papers and documentation, writing notes, etc.
However, in order to train and evaluate deep learning models, hardware for computationally intensive processes is required.

Therefore, access to a GPU cluster has been provided in order to train models using GPUs.
The Computer Vision Lab provides access to the BIWI cluster which has been used to store datasets, models and also for computation.
The BIWI cluster consists in 70 computational nodes (CPU only) with a total of 556 processors, 8328 cores and 13.3TB of RAM memory.
In addition, there are GPU nodes which consists in 75 GPUs with 12GB of GPU memory each.

\subsection{Software}

All the code had been developed using Python3.
Python is commonly used in computer vision and deep learning applications because most of the available libraries that provide computational frameworks provide a Python interface, and also due to the simple, straightforward way to develop code.

The main library used to develop deep learning models has been PyTorch~\pytorch{}.
This library provides high-level features for tensor computation and deep neural networks.
It is a Python wrapper which under the hood is written in C allowing fast performance and strong GPU acceleration.
This library is usually used in the research community due to its flexibility, fast code prototyping and debugging capabilities.

\section{Datasets}

During the development of this thesis, two segmentation datasets have been used: DAVIS~\davisboth{} and PASCAL~\pascal{}.
DAVIS is our main playground, used to evaluate our method on video object segmentation, while PASCAL was used as additional data for training, as it provides a huge set of instance annotations on images.

\subsection{DAVIS Dataset}

The DAVIS Dataset~\davisboth{} stands for Densely Annotated Video Segmentation dataset.
It provides curated dense annotations for object instances in video sequences.
There are two versions of the dataset: DAVIS 2016~\davisold{} and DAVIS 2017~\davislast{}.
The 2016 version provides foreground/background annotations while the 2017 version provides annotations for multiple objects and instances in the foreground.
During the work done in this thesis, the DAVIS 2017 version has been used.
Some examples of the annotations are shown in \figref{datasets:davis} and information about the dataset is also given in \tabref{datasets:davis}.

\begin{table}[h]
  \centering
  \begin{tabular}{l|llll|l}
    \toprule
    DAVIS 2017                          & train & val  & test-dev & test-challenge & \textbf{Total} \\
    \midrule
    Number of sequences                 & 60    & 30   & 30       & 30             & \textbf{50}    \\
    Number of frames                    & 4219  & 2023 & 2037     & 2180           & \textbf{10459} \\
    Mean number of frames per sequence  & 70.3  & 67.4 & 67.9     & 72.7           & \textbf{69.7}  \\
    Number of objects                   & 138   & 59   & 89       & 90             & \textbf{376}   \\
    Mean number of objects per sequence & 2.30  & 1.97 & 2.97     & 3.00           & \textbf{2.51} \\
    \bottomrule
  \end{tabular}
  \caption{Size of the DAVIS 2017~\davislast{} data splits: number of sequences, frames and annotated objects.}
  \label{tab:datasets:davis}
\end{table}

\begin{figure}[h]
  \centering
  \davisdatasetrow{bike-packing}{blackswan}{bmx-trees}{breakdance}{camel}
  \davisdatasetrow{car-roundabout}{car-shadow}{cows}{dance-twirl}{dog}
  \davisdatasetrow{dogs-jump}{drift-chicane}{drift-straight}{goat}{gold-fish}
  \davisdatasetrow{horsejump-high}{india}{judo}{kite-surf}{lab-coat}
  \davisdatasetrow{libby}{loading}{mbike-trick}{motocross-jump}{paragliding-launch}
  \davisdatasetrow{parkour}{pigs}{scooter-black}{shooting}{soapbox}
  \caption{First frame annotation for all the sequences in the validation subset of DAVIS 2017~\davislast{}. }
  \label{fig:datasets:davis}
\end{figure}


The DAVIS dataset has been used mainly in the two following setups:

\paragraph{Semi-supervised}

A mask in the first frame of a sequence is provided to the algorithms.
The task is to output a mask for the rest of the frames in the video sequence.
The initial mask gives information about which is the object intended to be segmented.

\paragraph{Unsupervised}

As the name specifies, no information is given about the object that must be segmented.
To solve this, the segmentation methods rely only on the unlabeled images of the video to infer the foreground and background.

The method presented in this thesis falls in the semi-supervised category but instead of using the full mask, a single point is used. Therefore, we call our approach weakly supervised.

To obtain weak labels for our videos, in this case point trajectories inside each sequence, we collected manual annotations from the members of CVL.
%The annotation was done using an existing tool developed inside the group by Dr.~Jordi Pont-Tuset.
It consisted in annotating a fixed point of a certain object along all the frames, moreover the visibility of the point was also annotated.
For example, a fixed point in an object can be an animal's eye, a person point on the head or the center of a wheel. No restrictions about the point to be tracked were requested by the annotators.

\subsection{PASCAL Dataset}

PASCAL Dataset~\pascal{} is a popular computer vision benchmark that consists of images with labels for 20 object classes and provides multiple annotations for different tasks like detection and segmentation.
The segmentation annotations consist of masks over instances belonging to the 20 classes, which leads to images with multiple instance annotations.

During this thesis, the segmentation annotations from PASCAL VOC 2012 has been used.
The information about the number of instances per split can be found on \tabref{datasets:pascal} and some examples of the dataset are shown in \figref{datasets:pascal}.

\begin{table}[h]
  \centering
  \begin{tabular}{l|ll|l}
  \toprule
  PASCAL VOC 2012                    & train & val  & \textbf{Total} \\
  \midrule
  Number of images                   & 1464  & 1449 & \textbf{2913}  \\
  Number of instances                & 3507  & 3427 & \textbf{6934}  \\
  Mean number of instances per image & 2.40  & 2.37 & \textbf{2.38}  \\
  \bottomrule
  \end{tabular}
  \caption{Size of the PASCAL VOC 2012~\pascal{} data splits: number of images and annotated objects.}
  \label{tab:datasets:pascal}
\end{table}


\begin{figure}[h]
  \centering
  % Images
  \pascaldatasetrow{2011_002575}{2007_001774}{2008_003329}{2008_001640}{2010_000372}
  \pascaldatasetrow{2008_005650}{2007_009655}{2009_003933}{2010_004550}{2009_004494}
  \pascaldatasetrow{2007_002954}{2009_001391}{2010_003798}{2008_004701}{2010_004577}
  \pascaldatasetrow{2010_001160}{2008_002504}{2011_000105}{2007_007591}{2007_008571}
  \pascaldatasetrow{2007_001677}{2010_004056}{2008_003926}{2011_000893}{2009_001197}
  \pascaldatasetrow{2008_005245}{2011_002767}{2007_000999}{2011_001082}{2008_000785}
  \caption{Example of images and annotations from PASCAL VOC 2012 Dataset~\pascal{}. }
  \label{fig:datasets:pascal}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}

In order to test the proposed method, we divide the experimentation in three steps.
The first step consists in testing the tracking of the points.
The second one consists in instance segmentation, assuming that a point belonging to an instance is given.
The last step consists in evaluating video sequences considering the weakly supervised setup.

\subsection{Tracking}

In order to perform tracking of a single point in a video sequence, we have developed two different approaches.
The first approach is inspired by OSVOS~\osvos{} where a model is finetuned for each sequence with the information available in the first frame.
In our case the point represented as a heatmap is being used and it is tested against the DAVIS 2017 validation subset.

% 'asses' is a very different thing!
To assess the performance of the algorithms, we have to define a metric.
For point tracking, the PCK metric~\pckmetric{} is chosen as it has been widely used in pose estimation to evaluate keypoint position prediction.
PCK metric illustrates, for a range of distance in pixels in the original resolution of the image (480p in our case), the detection rate.
At each distance in pixels, the detection rate can be seen as the percentage of samples where the prediction is as furthest this distance. % \ToDo{Rephrase this sentence}

In the following experiments, a ResNet50 backbone architecture has been used with a PSP module~\pspnet{} plugged on top.
No major difference was observed using a ResNet101 architecture so the ResNet50 was used due to its lower training time as it has less training parameters.
The resolution of the images fed to the network was $512 \times 896$ pixels.
Regarding the training parameters, SGD optimizer with a learning rate of $10^{-6}$, momentum $0.9$ and weight decay of $0.0005$ was used. The training for each sequence consisted in 3000 iterations with batch size of $2$.

To compare our results, we designed two baselines to compare with.
The first one consists of predicting a point at each frame of the sequence as the same point given at the first frame.
The second baseline consists of running the MDNet~\mdnet{} tracker on the smallest bounding box that fit the ground truth mask of the sequence.
The testing was performed comparing the center of the predicted bounding box against the ground truth bounding box.

We tested two variations of the model.
The first one used a single channel prediction trained with a heatmap of $\sigma = 40$ pixels.
The second variation consisted of a multiscale heatmap.
In the second setup, we trained using three channels with a heatmap in each one with a $\sigma$ equal to 24, 48 and 96 , respectively.
The prediction of the point was always performed by computing the position of the maximum value.
In the case of the last configuration, the three channels were summed along the channel dimension before predicting the point position.
The results are plotted in \figref{experiments:tracking:pointregression}.

\begin{figure}[h]
	\centering
	\input{plots/tracking_point_regression.tex}
  \caption{Precision performance for our proposed methods that track points regressing a heatmap. \ToDo{More?}}
  \label{fig:experiments:tracking:pointregression}
\end{figure}

In addition to this metric, in order to validate whether our model has been able to learn the representation of the object to track, we compute a coverage factor.
This coverage factor is defined as the accuracy of the model to predict a tracked point inside the ground truth mask of the object to track.
These results are shown in \tabref{experiments:tracking:coveragepointregression}.

\begin{table}[h]
  \centering
  \begin{tabular}{l|l}
    \toprule
    Method                       & Coverage       \\
    \midrule
    Baseline                     & 0.428          \\
    MDNET~\mdnet                 & 0.851          \\
    % Hourglass           & 0.812    \\
    % ResNet101           & 0.912 \\
    ResNet50 Finetune            & 0.912          \\
    ResNet50 Multiscale Finetune & \textbf{0.940} \\
    \bottomrule
  \end{tabular}
  \caption{Coverage for different methods that regress the heatmap.
  Coverage is defined as the accuracy of the model to predict a tracked point inside the ground truth mask of the object to track. \ToDo{More?}}
  \label{tab:experiments:tracking:coveragepointregression}
\end{table}

These experiments present good results in localization precision and coverage (at least we know that if the tracked point is not precise, it is consistent with the object to track).
The biggest setback of this approach is the computational complexity for training the model.
We need to train a model for each sequence which may lead to a big prediction time for new sequences (in the order of 10-30 minutes per sequence).

For this reason and in order to obtain a single model independent on the sequence to track, we use metric learning.
Given the ResNet model, we trained the architecture with pretrained weights on semantic segmentation, using the Triplet Loss.
We also tested the possibility to reduce the output dimensions by plugging a convolutional layer at the end and reducing the number of output channels into $D$ (same as the embedding dimension).
% .performing some training with the Triplet Loss and expanding this architecture with a convolutional head to reduce the embedding dimensions.\ToDo{Rephrase}

Training was performed using the Adagrad optimizer with learning rate $0.001$ for 100 epochs over PASCAL~\pascal{} dataset with batch size $8$.
The Triplet Loss margin was set to $\alpha = 0.7$ and $64$ triplets were sampled per image.
It was tested with the original embedding dimension $D=2048$, and then adding a convolutional head to reduce its dimensionality.
$D=128$ was the dimensionality that presented better results.
For these experiments, the only backbone architecture used was ResNet101 as it is the only one publicly available with pretrained weights on semantic segmentation.


\begin{figure}[h]
  \centering
  \input{plots/tracking_metric_learning.tex}
  \caption{Precision performance for pixel representation tracking.}
  \label{fig:experiments:tracking:metriclearning}
\end{figure}

As can be seen in \figref{experiments:tracking:metriclearning}, the localization precision is really poor, in comparison to the previous experiments.
In contrast, the coverage results in \tabref{experiments:tracking:coveragemetriclearning} increase once training is done and the dimensionality is being reduced. The last configuration almost achieves the same result with a dedicated network for each sequence.
Note that this evaluation is performed in DAVIS~\davislast{} validation while the trained model has not seen any images from this dataset as it has been trained using the PASCAL~\pascal{} dataset.


\begin{table}[h]
  \centering
  \begin{tabular}{l|l}
    \toprule
    Method                   & Coverage       \\
    \midrule
    Baseline                 & 0.428          \\
    Pre-Trained VOC $D=2048$ & 0.620          \\
    Triplet Loss $D=2048$    & 0.750          \\
    Triplet Loss $D=128$     & \textbf{0.911} \\
    % Double Triplet Loss $D=2048$ & 0.885    \\
    \bottomrule
  \end{tabular}
  \caption{Coverage for different tracking methods trained with metric learning. }
  \label{tab:experiments:tracking:coveragemetriclearning}
\end{table}

From the last model trained using the Triplet Loss at $D=128$, we can conclude that the embeddings obtained for each pixel are representative for the instance the pixels belongs to, but not much from the specific point of the object.
Even though the tracking performance is not good enough, we think that the good coverage and the good embedding quality can make this architecture valid for both tracking and segmentation using a single model which is sequence independent.
That is the reason why we move forward with this model in the following experiments.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Instance Segmentation}

In order to perform instance segmentation, we tested the model that was trained using the Triplet Loss.
Given a single point per object, we obtain a mask corresponding to that object.
To evaluate our model for this task, the PASCAL~\pascal{} dataset has been used.
The point used to test the segmentation is obtained by computing the distance to the ground truth mask contour and taking the coordinate which maximizes this distance.
This point has an embedding representation that is descriptive enough for the instance to be segmented.

As the Triplet Loss only maximizes and minimizes the distance of the embedding pairs, no metric is used during the training apart from the validation loss.
However, a qualitative visualization can be performed to check the quality of the obtained embeddings.
For this visualization, the embeddings for each image are computed  and then a PCA decomposition over the embeddings of all pixels is done.
The PCA decomposition is performed to reduce the dimensionality to 3 dimensions and the values of this 3 dimensions are scaled to obtain a RGB image.
In \figref{experiments:segmentation:pca}, some examples of PASCAL~\pascal{} validation images are shown.
On this projection, we can observe in most of the cases how different color tones show differences between instances even for the same class.

\begin{figure}[h]
  \centering
  \showpascalpca{2007_000042}
  \showpascalpca{2007_000129}
  \showpascalpca{2007_000464}
  \showpascalpca{2007_001239}
  \showpascalpca{2007_005331}
  \showpascalpca{2008_007273}
  \showpascalpca{2009_002539}
  % \showpascalpca{2009_005219}
  \showpascalpca{2010_002868}
  \caption{PASCAL~\pascal{} validation images where the embeddings have been projected to RGB using PCA.}
  \label{fig:experiments:segmentation:pca}
\end{figure}

The next step is to evaluate the instance segmentation given a point per instance.
As we have a one-to-one matching between the ground truth masks and the predicted masks, we use the Mean Intersection over Union ($mIoU$) as the evaluation metric.
It is computed as follows, $\mathcal{P}$ being the set of pixels for which the prediction mask is $1$ and $\mathcal{G}$ the set of pixels which are annotated as $1$:

\begin{equation}
  mIoU = \frac{|\mathcal{P} \cap \mathcal{G}|}{|\mathcal{P} \cup \mathcal{G}|}
\end{equation}

The segmentation given a point is computed as explained in \secref{methods:embeddingsegmentation}.
Therefore, given a point, we obtain the embedding of the pixel $x_k$.
The next step is to compute a distance metric between this pixel embedding $x_k$ and the rest of the pixels in the image $\mathcal{X}$.
We have observed that the distance function (using the euclidean distance or the cosine distance which are commonly used for embedding similarity) does not present any difference on the results.
Thus, in all future steps the euclidean distance is used.

Then, the output is a distance map and a simple thresholding operation is able to return the mask for each instance.
Some examples of results are shown in \figref{experiments:segmentation:distancemaps} where the images and the ground truths are in the left column.
In the middle column, the distance maps for different instances in the image are shown and in the right column, the predicted masks after thresholding the distance maps are displayed.
For this example, the used threshold is $\gamma = 0.5$.

\begin{figure}[h]
  \centering
  \showpascaldistancemap{2007_005331}
  \showpascaldistancemap{2007_001239}
  \showpascaldistancemap{2007_006698}
  \caption{Instance segmentation procedure computing the distance map.
    \textbf{Left}: Image and ground truth annotation for all objects in the image.
    \textbf{Middle}: Distance map for two instances in the image. Darker means lower distance and lighter means higher distance.
    \textbf{Right}: Predicted mask for each instance after thresholding the distance map.
  }
  \label{fig:experiments:segmentation:distancemaps}
\end{figure}

We have observed that the value of the margin $\alpha$ used at training with the metric loss has a large impact on the performance of the segmentation.
In order to test different margin values $\alpha$, the result of several experiments is shown in \tabref{experiments:segmentation:marginmiou}.
We can conclude that the best value to train the model and obtain a good segmentation result is $\alpha = .7$.

\begin{table}[h]
  \centering
  \begin{tabular}{r|ccccccc}
    \toprule
    Margin $\alpha$ & $.5$    & $\mathbf{.7}$    & $.8$    & $1.0$   & $1.2$   & $1.5$   & $2.0$   \\
    \midrule
    $mIoU$          & $.6351$ & $\mathbf{.6464}$ & $.6385$ & $.5620$ & $.5825$ & $.4834$ & $.3977$ \\
    \bottomrule
  \end{tabular}
  \caption{Mean intersection over union against the Triplet Loss margin $\alpha$ used to train (ResNet101 with a convolution head and $D=128$). }
  \label{tab:experiments:segmentation:marginmiou}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Video Object Segmentation}

In the last experiments section, we put everything together, by fusing the two previous approaches.
We have tested the performance of instance segmentation on DAVIS~\davislast{} 2017 in the sequences of the validation subset.
Our approach is compared against OnAVOS~\onavos{} and OSVOS~\osvos{} which are state-of-the-art methods that made their predictions publicly available in the DAVIS~\davislast{} 2017 dataset. We compare these two methods against different configurations of our method.

In the first configuration, we compute the segmentation using perfect keypoints (keypoints obtained from the ground truth mask) which in this case the segmentation method is being tested.
The second approach consists of segmenting each frame of the sequence using the embedding of the keypoint given in the first frame.
The third and last configuration is finding the pixel at each frame which has the higher similarity to the annotated pixel in the first frame.
Then, for each frame, using the keypoint tracked from the first frame, segmentation is done as explained in \secref{methods:embeddingsegmentation}.
The results in terms of mIoU are shown in \tabref{experiments:videosegmentation:davismiou}.

\begin{table}[h]
  \centering
  \begin{tabular}{l|l}
    \toprule
    Method                       & mIoU  \\
    \midrule
    OnAVOS~\onavos{}             & 0.616 \\
    OSVOS~\osvos{}               & 0.566 \\
    \midrule
    Perfect Keypoints            & 0.610 \\
    First Frame Keypoint         & 0.426 \\
    First Frame Keypoint tracked & 0.432 \\
    \bottomrule
  \end{tabular}
  \caption{Overall performance for video object segmentation comparing different methods and our different approaches.\ToDo{More?}}
  \label{tab:experiments:videosegmentation:davismiou}
\end{table}

\begin{figure}[H]
  \centering
  \input{plots/miou_sequence.tex}
  \caption{Per sequence results on DAVIS~\davislast{} 2017 validation. }
  \label{fig:experiments:videosegmentation:persequencemiou}
\end{figure}

As can be seen in \tabref{experiments:videosegmentation:davismiou}, the segmentation using the ground truth to compute a point in every frame, leads to a similar performance compared to the state of the art.
In contrast, when only the information from the first frame is used, the performance drops considerably.
Nevertheless, the tracking operation gives a little boost in comparison to only using the first frame embedding.

If we further analyze the results, the per sequence performance when tracking the keypoints is shown in \figref{experiments:videosegmentation:persequencemiou}.
Here, we can observe the a big disparity of results among sequences.
It seems that the biggest challenge for our method is when the object in the first frame is small.
Throughout the experiments, we have already found out that a drop in performance of the embeddings in small objects and also when they are very close to the contour so very thin objects (e.g. bicycles) present very bad performance.
In addition, the prediction of the mask for different sequences are plotted in \figref{experiments:videosegmentation:predmasksdavis}.

\begin{figure}[ht]
  \centering
  \showdavisresults{car-roundabout}
  \showdavisresults{dog}
  \showdavisresults{camel}
  \showdavisresults{judo}
  \showdavisresults{loading}
  \showdavisresults{libby}
  \showdavisresults{pigs}
  \showdavisresults{breakdance}
  \showdavisresults{gold-fish}
  \showdavisresults{bmx-trees}
  \showdavisresults{dogs-jump}
  \showdavisresults{paragliding-launch}
  \showdavisresults{soapbox}
  \caption{Mask prediction results on some sequences in the DAVIS~\davislast{} 2017 validation set. }
  \label{fig:experiments:videosegmentation:predmasksdavis}
\end{figure}
