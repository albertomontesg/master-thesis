%!TEX root = ../thesis.tex

\chapter*{Abstract}
\label{cha:abstract}

In this thesis, we tackle the problem of video object segmentation where we have to give the mask of every annotated object.
Our implementation relies on a single point annotated on the first frame per object in order to obtain its mask.

To solve this problem, we present two subtasks to obtain the desired output: 
track a point through the video,
and given a point for an instance obtain its segmentation mask.
We present a Fully Convolutiona Neural Network model that learn an embedding representation for every image pixel.
This embedding representation is trained using the Triplet Loss which performs the task of learning a good embedding using distance comparison.
Also, this embedding is used to track points and obtain segmentation masks with the use of similarity between embeddings.

The presented method presents promising results on segmentation while leaves some margin for improvement on the task of tracking object points.
